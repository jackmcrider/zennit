{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810f3c46",
   "metadata": {},
   "source": [
    "# Image Classification with VGG\n",
    "\n",
    "This tutorial will introduce the attribution of image classifiers using VGG11\n",
    "and ResNet18 on ImageNet. Feel free to replace VGG11 and ResNet18 with any\n",
    "other version of VGG or ResNet respectively.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "First, we install **Zennit**. This includes its dependencies `Pillow`,\n",
    "`torch` and `torchvision`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install zennit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2dc4cd",
   "metadata": {},
   "source": [
    "Then, we import necessary modules, classes and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a3fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "from torchvision.models import vgg11_bn, resnet18\n",
    "\n",
    "from zennit.attribution import Gradient, SmoothGrad\n",
    "from zennit.composites import EpsilonGammaBox, EpsilonPlusFlat\n",
    "from zennit.composites import SpecialFirstLayerMapComposite, NameMapComposite\n",
    "from zennit.image import imgify, imsave\n",
    "from zennit.rules import Epsilon, ZPlus, Norm, Pass, Flat\n",
    "from zennit.types import Convolution, Activation, AvgPool\n",
    "from zennit.torchvision import VGGCanonizer, ResNetCanonizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e434e",
   "metadata": {},
   "source": [
    "We download an image of the [Dornbusch\n",
    "Lighthouse](https://en.wikipedia.org/wiki/Dornbusch_Lighthouse) from [Wikimedia\n",
    "Commons](https://commons.wikimedia.org/wiki/File:2006_09_06_180_Leuchtturm.jpg):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.hub.download_url_to_file(\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8b/2006_09_06_180_Leuchtturm.jpg/640px-2006_09_06_181_Leuchtturm.jpg',\n",
    "    'dornbusch-lighthouse.jpg',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798a0c1",
   "metadata": {},
   "source": [
    "We load and prepare the data. The image is resized such that the shorter side\n",
    "is 256 pixels in size, then center-cropped to `(224, 224)`, converted to a\n",
    "`torch.Tensor`, and then normalized according the channel-wise mean and\n",
    "standard deviation of the ImageNet dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base image transform\n",
    "transform_img = Compose([\n",
    "    Resize(256),\n",
    "    CenterCrop(224),\n",
    "])\n",
    "# define the normalization transform\n",
    "transform_norm = Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "# define the full tensor transform\n",
    "transform = Compose([\n",
    "    transform_img,\n",
    "    ToTensor(),\n",
    "    transform_norm,\n",
    "])\n",
    "\n",
    "# load the image\n",
    "image = Image.open('dornbusch-lighthouse.jpg')\n",
    "\n",
    "# transform the PIL image and insert a batch-dimension\n",
    "data = transform(image)[None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b4dd8",
   "metadata": {},
   "source": [
    "We can look at the original image and the cropped image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the original image\n",
    "display(image)\n",
    "# display the resized and cropped image\n",
    "display(transform_img(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d45bd9b",
   "metadata": {},
   "source": [
    "## VGG11 without BatchNorm\n",
    "\n",
    "We start with VGG11 without BatchNorm.\n",
    "First, we initialize the VGG16 model and load the hyperparameters. Set\n",
    "`pretrained=True` to use the pre-trained model instead of the random one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853bcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and set it to evaluation mode\n",
    "model = vgg11(pretrained=False).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca56ed",
   "metadata": {},
   "source": [
    "### VGG11 - Sensitivity Analysis\n",
    "We first compute the Sensitivity-Analysis attribution, which is the squared\n",
    "gradient, by using the *Gradient* **Attributor**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the attributor, specifying model\n",
    "with Gradient(model=model) as attributor:\n",
    "    # compute the model output and attribution\n",
    "    output, attribution = attributor(data, target)\n",
    "\n",
    "print(f'Prediction: {output.argmax(1)[0].item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b5056",
   "metadata": {},
   "source": [
    "Visualize the attribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb87c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# square and sum over the channels\n",
    "relevance = (attribution ** 2).sum(1)\n",
    "\n",
    "# create an image of the visualize attribution the relevance is only\n",
    "# positive, so we use symmetric=False and an unsigned color-map\n",
    "img = imgify(relevance, symmetric=False, cmap='hot')\n",
    "\n",
    "# show the image\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e53bad",
   "metadata": {},
   "source": [
    "### VGG11 - SmoothGrad using Attributors\n",
    "Model-agnostic attribution methods like *SmootGrad* are implemented as *Attributors*.\n",
    "For these, we simply replace the `Gradient` **Attributor** with the appropriate **Attributor**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8846f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the attributor, specifying model\n",
    "with SmoothGrad(noise_level=0.1, n_iter=20, model=model) as attributor:\n",
    "    # compute the model output and attribution\n",
    "    output, attribution = attributor(data, target)\n",
    "\n",
    "print(f'Prediction: {output.argmax(1)[0].item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad8f723",
   "metadata": {},
   "source": [
    "Visualize the attribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb34a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the absolute and sum over the channels\n",
    "relevance = attribution.abs().sum(1)\n",
    "\n",
    "# create an image of the visualize attribution the relevance is only\n",
    "# positive, so we use symmetric=False and an unsigned color-map\n",
    "img = imgify(relevance, symmetric=False, cmap='hot')\n",
    "\n",
    "# show the image\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0510255",
   "metadata": {},
   "source": [
    "### VGG11 - Layer-wise Relevance Propagation (LRP) with EpsilonPlusFlat\n",
    "We compute the LRP-attribution also using the *Gradient* **Attributor**\n",
    "together with the `EpsilonPlusFlat` **Composite**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1645b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a composite\n",
    "composite = EpsilonPlusFlat()\n",
    "\n",
    "# choose a target class for the attribution (label 437 is lighthouse)\n",
    "target = torch.eye(1000)[[437]]\n",
    "\n",
    "# create the attributor, specifying model and composite\n",
    "with Gradient(model=model, composite=composite) as attributor:\n",
    "    # compute the model output and attribution\n",
    "    output, attribution = attributor(data, target)\n",
    "\n",
    "print(f'Prediction: {output.argmax(1)[0].item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e9ba55",
   "metadata": {},
   "source": [
    "Visualize the attribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over the channels\n",
    "relevance = attribution.sum(1)\n",
    "\n",
    "# create an image of the visualize attribution\n",
    "img = imgify(relevance, symmetric=True, cmap='coldnhot')\n",
    "\n",
    "# show the image\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9169bb3",
   "metadata": {},
   "source": [
    "### VGG11 - Layer-wise Relevance Propagation (LRP) with EpsilonGammaBox\n",
    "We now compute the LRP-attribution with the `EpsilonGammaBox` **Composite**.\n",
    "The `EpsilonGammaBox` **Composite** uses the `ZBox` rule, which needs as\n",
    "arguments the lowest and highest possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17da931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the VGG-specific canonizer (alias for SequentialMergeBatchNorm, only\n",
    "# needed with batch-norm)\n",
    "canonizer = VGGCanonizer()\n",
    "\n",
    "# the EpsilonGammaBox composite needs the lowest and highest values, which are\n",
    "# here for ImageNet 0. and 1. with a different normalization for each channel\n",
    "low, high = transform_norm(torch.tensor([[[[[0.]]] * 3], [[[[1.]]] * 3]]))\n",
    "\n",
    "# create a composite, specifying arguments and the canonizers, if any\n",
    "composite = EpsilonGammaBox(low=low, high=high, canonizers=[canonizer])\n",
    "\n",
    "# choose a target class for the attribution (label 437 is lighthouse)\n",
    "target = torch.eye(1000)[[437]]\n",
    "\n",
    "# create the attributor, specifying model and composite\n",
    "with Gradient(model=model, composite=composite) as attributor:\n",
    "    # compute the model output and attribution\n",
    "    output, attribution = attributor(data, target)\n",
    "\n",
    "print(f'Prediction: {output.argmax(1)[0].item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a14c5",
   "metadata": {},
   "source": [
    "Visualize the attribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1032120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over the channels\n",
    "relevance = attribution.sum(1)\n",
    "\n",
    "# create an image of the visualize attribution\n",
    "img = imgify(relevance, symmetric=True, cmap='coldnhot')\n",
    "\n",
    "# show the image\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e742f2",
   "metadata": {},
   "source": [
    "### More Visualization\n",
    "We can try out different color-maps by either using another built-in color map, or using the color-map specification language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Built-in color-map bwr')\n",
    "display(imgify(relevance, symmetric=True, cmap='bwr'))\n",
    "\n",
    "print('CMLS code for color-map from cyan to grey to purple')\n",
    "display(imgify(relevance, symmetric=True, cmap='0ff,444,f0f'))\n",
    "\n",
    "print('CMSL code for grey scale for negative and red for positive values')\n",
    "display(imgify(relevance, symmetric=True, cmap='fff,000,f00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bf7029",
   "metadata": {},
   "source": [
    "To directly save the visualized attribution, we can use `imsave` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c2b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly save the visualized attribution\n",
    "imsave('attrib-1.png', relevance, symmetric=True, cmap='bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b6dcd1",
   "metadata": {},
   "source": [
    "## VGG11 with BatchNorm\n",
    "\n",
    "VGG11 with BatchNorm requires a canonizer for LRP to function properly.\n",
    "We initialize the VGG11-bn model and load the hyperparameters. Set\n",
    "`pretrained=True` to use the pre-trained model instead of the random one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b50d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and set it to evaluation mode\n",
    "model = vgg11_bn(pretrained=False).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f677171",
   "metadata": {},
   "source": [
    "### VGG11-bn - Layer-wise Relevance Propagation (LRP) with EpsilonGammaBox\n",
    "We now compute the LRP-attribution with the `EpsilonGammaBox` **Composite**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the VGG-specific canonizer (alias for SequentialMergeBatchNorm, only\n",
    "# needed with batch-norm)\n",
    "canonizer = VGGCanonizer()\n",
    "\n",
    "# the EpsilonGammaBox composite needs the lowest and highest values, which are\n",
    "# here for ImageNet 0. and 1. with a different normalization for each channel\n",
    "low, high = transform_norm(torch.tensor([[[[[0.]]] * 3], [[[[1.]]] * 3]]))\n",
    "\n",
    "# create a composite, specifying arguments and the canonizers, if any\n",
    "composite = EpsilonGammaBox(low=low, high=high, canonizers=[canonizer])\n",
    "\n",
    "# choose a target class for the attribution (label 437 is lighthouse)\n",
    "target = torch.eye(1000)[[437]]\n",
    "\n",
    "# create the attributor, specifying model and composite\n",
    "with Gradient(model=model, composite=composite) as attributor:\n",
    "    # compute the model output and attribution\n",
    "    output, attribution = attributor(data, target)\n",
    "\n",
    "print(f'Prediction: {output.argmax(1)[0].item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db70111",
   "metadata": {},
   "source": [
    "Visualize the attribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ced93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over the channels\n",
    "relevance = attribution.sum(1)\n",
    "\n",
    "# create an image of the visualize attribution\n",
    "img = imgify(relevance, symmetric=True, cmap='coldnhot')\n",
    "\n",
    "# show the image\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86e028",
   "metadata": {},
   "source": [
    "### VGG11-bn - Layer-wise Relevance Propagation (LRP) with Custom NameMapComposite\n",
    "We demonstrate how rules can be assigned on a name-base using the abstract\n",
    "`NameMapComposite`. Note that this particular case will only work with\n",
    "VGG11-bn, so we redefine the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and set it to evaluation mode\n",
    "model = vgg11_bn(pretrained=False).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c788424",
   "metadata": {},
   "source": [
    "We can take a look at all leaf modules by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f00750",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = [\n",
    "    (name, module)\n",
    "    for name, module in model.named_modules()\n",
    "    if not [*islice(module.children(), 1)]\n",
    "]\n",
    "maxlen = max(len(name) for name, _ in leaves)\n",
    "print('\\n'.join(f'{name:{maxlen}s}: {module}' for name, module in leaves))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5cff96",
   "metadata": {},
   "source": [
    "We can look at the architecture and the assign the rules directly to the names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38edd796",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_map = [\n",
    "    (['features.0'], Flat()),  # Conv2d\n",
    "    (['features.1'], Pass()),  # BatchNorm2d\n",
    "    (['features.2'], Pass()),  # ReLU\n",
    "    (['features.3'], Norm()),  # MaxPool2d\n",
    "    (['features.4'], ZPlus()),  # Conv2d\n",
    "    (['features.5'], Pass()),  # BatchNorm2d\n",
    "    (['features.6'], Pass()),  # ReLU\n",
    "    (['features.7'], Norm()),  # MaxPool2d\n",
    "    (['features.8'], ZPlus()),  # Conv2d\n",
    "    (['features.9'], Pass()),  # BatchNorm2d\n",
    "    (['features.10'], Pass()),  # ReLU\n",
    "    (['features.11'], ZPlus()),  # Conv2d\n",
    "    (['features.12'], Pass()),  # BatchNorm2d\n",
    "    (['features.13'], Pass()),  # ReLU\n",
    "    (['features.14'], Norm()),  # MaxPool2d\n",
    "    (['features.15'], ZPlus()),  # Conv2d\n",
    "    (['features.16'], Pass()),  # BatchNorm2d\n",
    "    (['features.17'], Pass()),  # ReLU\n",
    "    (['features.18'], ZPlus()),  # Conv2d\n",
    "    (['features.19'], Pass()),  # BatchNorm2d\n",
    "    (['features.20'], Pass()),  # ReLU\n",
    "    (['features.21'], Norm()),  # MaxPool2d\n",
    "    (['features.22'], ZPlus()),  # Conv2d\n",
    "    (['features.23'], Pass()),  # BatchNorm2d\n",
    "    (['features.24'], Pass()),  # ReLU\n",
    "    (['features.25'], ZPlus()),  # Conv2d\n",
    "    (['features.26'], Pass()),  # BatchNorm2d\n",
    "    (['features.27'], Pass()),  # ReLU\n",
    "    (['features.28'], Norm()),  # MaxPool2d\n",
    "    (['avgpool'], Norm()),  # AdaptiveAvgPool2d\n",
    "    (['classifier.0'], Epsilon()),  # Linear\n",
    "    (['classifier.1'], Pass()),  # ReLU\n",
    "    (['classifier.2'], Pass()),  # Dropout\n",
    "    (['classifier.3'], Epsilon()),  # Linear\n",
    "    (['classifier.4'], Pass()),  # ReLU\n",
    "    (['classifier.5'], Pass()),  # Dropout\n",
    "    (['classifier.6'], Epsilon()),  # Linear\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7580d344",
   "metadata": {},
   "source": [
    "We can use that name-map to create the composite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e325b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the VGG-specific canonizer (alias for SequentialMergeBatchNorm, only\n",
    "# needed with batch-norm)\n",
    "canonizer = VGGCanonizer()\n",
    "\n",
    "# create a composite, specifying arguments and the canonizers, if any\n",
    "composite = NameMapComposite(\n",
    "    name_map=name_map,\n",
    "    canonizers=[canonizer],\n",
    ")\n",
    "\n",
    "# choose a target class for the attribution (label 437 is lighthouse)\n",
    "target = torch.eye(1000)[[437]]\n",
    "\n",
    "# create the attributor, specifying model and composite\n",
    "with Gradient(model=model, composite=composite) as attributor:\n",
    "    # compute the model output and attribution\n",
    "    output, attribution = attributor(data, target)\n",
    "\n",
    "print(f'Prediction: {output.argmax(1)[0].item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352c0883",
   "metadata": {},
   "source": [
    "Visualize the attribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8182b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over the channels\n",
    "relevance = attribution.sum(1)\n",
    "\n",
    "# create an image of the visualize attribution\n",
    "img = imgify(relevance, symmetric=True, cmap='coldnhot')\n",
    "\n",
    "# show the image\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba184117",
   "metadata": {},
   "source": [
    "## ResNet18\n",
    "Then, we initialize the ResNet18 model and load the hyperparameters. Set\n",
    "`pretrained=True` to use the pre-trained model instead of the random one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and set it to evaluation mode\n",
    "model = resnet18(pretrained=False).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dee524",
   "metadata": {},
   "source": [
    "### ResNet18 - LRP with EpsilonPlusFlat\n",
    "Compute the LRP-attribution using the ``EpsilonPlusFlat`` **Composite**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2cd10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the ResNet-specific canonizer\n",
    "canonizer = ResNetCanonizer()\n",
    "\n",
    "# create a composite, specifying the canonizers, if any\n",
    "composite = EpsilonPlusFlat(canonizers=[canonizer])\n",
    "\n",
    "# choose a target class for the attribution (label 437 is lighthouse)\n",
    "target = torch.eye(1000)[[437]]\n",
    "\n",
    "# create the attributor, specifying model and composite\n",
    "with Gradient(model=model, composite=composite) as attributor:\n",
    "    # compute the model output and attribution\n",
    "    output, attribution = attributor(data, target)\n",
    "\n",
    "print(f'Prediction: {output.argmax(1)[0].item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a7f06",
   "metadata": {},
   "source": [
    "Visualize the attribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6021bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over the channels\n",
    "relevance = attribution.sum(1)\n",
    "\n",
    "# create an image of the visualize attribution\n",
    "img = imgify(relevance, symmetric=True, cmap='coldnhot')\n",
    "\n",
    "# show the image\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9230131a",
   "metadata": {},
   "source": [
    "### ResNet18 - LRP with a Custom LayerMapComposite\n",
    "In this demonstration, we want to ignore the BatchNorm layers and attribute the\n",
    "residual connections not by contribution but equally. This setup requires no **Canonizer**, but a special Composite which ignores the BatchNorm modules for LRP by assigning them the `Pass` rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ZBox rule needs the lowest and highest values, which are here for\n",
    "# ImageNet 0. and 1. with a different normalization for each channel\n",
    "low, high = transform_norm(torch.tensor([[[[[0.]]] * 3], [[[[1.]]] * 3]]))\n",
    "\n",
    "# create a composite, specifying the canonizers, if any\n",
    "composite = SpecialFirstLayerMapComposite(\n",
    "    # the layer map is a list of tuples, where the first element is the target\n",
    "    # layer type, and the second is the rule template\n",
    "    layer_map=[\n",
    "        (Activation, Pass()),  # ignore activations\n",
    "        (AvgPool, Norm()),  # normalize relevance for any AvgPool\n",
    "        (Convolution, ZPlus()),  # any convolutional layer\n",
    "        (Linear, Epsilon(epsilon=1e-6))  # this is the dense Linear, not any\n",
    "        (BatchNorm, Pass())  # ignore BatchNorm\n",
    "    ]\n",
    "    # the first map is only used once, to the first module which applies to the\n",
    "    # map, i.e. here the first layer of type AnyLinear\n",
    "    first_map = [\n",
    "        (AnyLinear, ZBox(low, high))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# choose a target class for the attribution (label 437 is lighthouse)\n",
    "target = torch.eye(1000)[[437]]\n",
    "\n",
    "# create the attributor, specifying model and composite\n",
    "with Gradient(model=model, composite=composite) as attributor:\n",
    "    # compute the model output and attribution\n",
    "    output, attribution = attributor(data, target)\n",
    "\n",
    "print(f'Prediction: {output.argmax(1)[0].item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3573e7bf",
   "metadata": {},
   "source": [
    "Visualize the attribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e17ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over the channels\n",
    "relevance = attribution.sum(1)\n",
    "\n",
    "# create an image of the visualize attribution\n",
    "img = imgify(relevance, symmetric=True, cmap='coldnhot')\n",
    "\n",
    "# show the image\n",
    "display(img)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
